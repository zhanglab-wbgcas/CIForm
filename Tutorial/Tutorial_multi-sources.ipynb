{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f964c2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Import the required packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score,f1_score,recall_score,precision_score\n",
    "import math\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import (DataLoader,Dataset)\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "##Set random seeds\n",
    "def same_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "    # Torch\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "same_seeds(2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6e50b1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Gene embedding, \n",
    "#Function\n",
    "    #The pre-processed scRNA-seq data is converted into a form acceptable to the Transformer encoder\n",
    "#Parameters \n",
    "    #gap: The length of a sub-vector\n",
    "    #adata: pre-processed scRNA-seq data. The rows represent the cells and the columns represent the genes\n",
    "    #Traindata_paths: the paths of the cell type labels file(.csv) corresponding to the training data\n",
    "def getXY(gap, adata, Traindata_paths):\n",
    "    # Converting the gene expression matrix into sub-vectors\n",
    "    #(n_cells,n_genes) -> (n_cells,gap_num,gap)  gap_num = int(gene_num / gap) + 1\n",
    "    X = adata.X\n",
    "    single_cell_list = []\n",
    "    for single_cell in X:\n",
    "        feature = []\n",
    "        length = len(single_cell)\n",
    "        for k in range(0, length, gap):\n",
    "            if (k + gap <= length):\n",
    "                a = single_cell[k:k + gap]\n",
    "            else:\n",
    "                a = single_cell[length - gap:length]\n",
    "\n",
    "            a = preprocessing.scale(a)\n",
    "            feature.append(a)\n",
    "        feature = np.asarray(feature)\n",
    "        single_cell_list.append(feature)\n",
    "\n",
    "    single_cell_list = np.asarray(single_cell_list)\n",
    "    \n",
    "    ##Obtaining cell types and all cell type labels of training data\n",
    "    if(Traindata_paths != None):\n",
    "        y_trains = []\n",
    "        for path in Traindata_paths:\n",
    "            y_train = pd.read_csv(path + \"Labels.csv\")\n",
    "            y_train = y_train.T\n",
    "            y_train = y_train.values[0]\n",
    "            y_trains.extend(y_train)\n",
    "\n",
    "        cell_types = []\n",
    "        for i in y_trains:\n",
    "            i = str(i).upper()\n",
    "            if (not cell_types.__contains__(i)):\n",
    "                cell_types.append(i)\n",
    "\n",
    "        return single_cell_list, y_trains, cell_types\n",
    "    else:\n",
    "        return single_cell_list\n",
    "\n",
    "##Function\n",
    "    #Converting label annotation to numeric form\n",
    "##Parameters\n",
    "    #cells:  all cell type labels\n",
    "    #cell_types: all cell types of Training datasets\n",
    "def getNewData(cells, cell_types):\n",
    "    labels = []\n",
    "    for i in range(len(cells)):\n",
    "        cell = cells[i]\n",
    "        cell = str(cell).upper()\n",
    "\n",
    "        if (cell_types.__contains__(cell)):\n",
    "            indexs = cell_types.index(cell)\n",
    "            labels.append(indexs + 1)\n",
    "        else:\n",
    "            labels.append(0)  # 0 denotes the unknowns cell types\n",
    "\n",
    "    return np.asarray(labels)\n",
    "\n",
    "##Function\n",
    "    #getting the input of CIForm\n",
    "##Parameters\n",
    "    #gap              :the length of a sub-vector \n",
    "    #Traindata_paths  :the paths of training datasets\n",
    "    #Train_names      :the names of training datasets \n",
    "    #Testdata_path    :the path of testing dataset\n",
    "    #Testdata_name    :the name of testing dataset\n",
    "    #topgenes         :the number of highly variable genes(HVGs)\n",
    "def getData(gap, Traindata_paths, Train_names,\n",
    "            Testdata_path, Testdata_name, topgenes):\n",
    "    all_adata = sc.AnnData\n",
    "    train_adata = sc.AnnData\n",
    "    for sa in range(0, len(Train_names)):\n",
    "        #getting the training datasets and making var names unique\n",
    "        temp_adata = sc.read_csv(Traindata_paths[sa] + Train_names[sa] + \".csv\", first_column_names=True)\n",
    "        temp_adata.var_names = [str(i).upper() for i in temp_adata.var_names]\n",
    "        temp_adata.var_names_make_unique()\n",
    "        \n",
    "        #Preprocessing the scRNA-seq data\n",
    "        sc.pp.filter_genes(temp_adata, min_cells=1)\n",
    "        sc.pp.normalize_total(temp_adata)\n",
    "        sc.pp.log1p(temp_adata)\n",
    "        \n",
    "        #integrating multiple training RNA-Seq datasets in the inner way to obtain their common genes\n",
    "        train_adata = train_adata.concatenate(temp_adata)\n",
    "        train_adata.var_names_make_unique()\n",
    "    Trainadata_num = len(train_adata)\n",
    "    \n",
    "    #Getting the testing dataset and making var names unique\n",
    "    test_adata = sc.read_csv(Testdata_path + Testdata_name + \".csv\")\n",
    "    test_adata.var_names_make_unique()\n",
    "    #Preprocessing the scRNA-seq data\n",
    "    sc.pp.filter_genes(test_adata, min_cells=1)\n",
    "    sc.pp.normalize_total(test_adata)\n",
    "    sc.pp.log1p(test_adata)\n",
    "    \n",
    "    #Integrating the training dataset and testing dataset in the inner way to obtain their common genes\n",
    "    all_adata = all_adata.concatenate(train_adata)\n",
    "    all_adata = all_adata.concatenate(test_adata)\n",
    "    \n",
    "    \n",
    "    #If the number of HVGs is greater than the number of common genes in the training dataset and the testing dataset, \n",
    "    #the number of HVGs is set as the number of common genes\n",
    "    width = all_adata.X.shape[1]\n",
    "    if (width < topgenes):\n",
    "        topgenes = width\n",
    "    \n",
    "    #Obtaining the HVGs\n",
    "    sc.pp.highly_variable_genes(all_adata, n_top_genes=topgenes)\n",
    "    all_adata.raw = all_adata\n",
    "    all_adata = all_adata[:, all_adata.var.highly_variable]\n",
    "    \n",
    "    #obtaining the pre-processed training dataset and test dataset \n",
    "    Train_adata = all_adata[:Trainadata_num]\n",
    "    Test_adata = all_adata[Trainadata_num:]\n",
    "\n",
    "    del all_adata\n",
    "    \n",
    "    #Converting the pre-processed training dataset into the input of Transformer Encoder and \n",
    "    #obtaining its cell type annotations and cell types\n",
    "    train_data, train_cells, train_cellTypes = getXY(gap, Train_adata, Traindata_paths)\n",
    "    \n",
    "    #Converting the preprocessed test dataset into the input of Transformer Encoder \n",
    "    Testdata_paths = []\n",
    "    Testdata_paths.append(Testdata_path)\n",
    "    test_data = getXY(gap, Test_adata, Label_path = None)\n",
    "    cell_types = train_cellTypes\n",
    "    \n",
    "    ##Converting cell type annotation of the training dataset to numeric form\n",
    "    Train_labels = getNewData(train_cells, cell_types)\n",
    "\n",
    "    return train_data, Train_labels, test_data, cell_types\n",
    "\n",
    "\n",
    "class TrainDataSet(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.length = len(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = torch.from_numpy(self.data)\n",
    "        label = torch.from_numpy(self.label)\n",
    "\n",
    "        return data[index], label[index]\n",
    "\n",
    "class TestDataSet(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "        self.length = len(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = torch.from_numpy(self.data)\n",
    "        return data[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f627b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Positional Encoder Layer\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        ##the sine function is used to represent the odd-numbered sub-vectors\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        ##the cosine function is used to represent the even-numbered sub-vectors\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "##CIForm\n",
    "##function\n",
    "    #annotating cell type identification of scRNA-seq data\n",
    "##parameters\n",
    "    #input_dim  :Default is equal to gap\n",
    "    #nhead      :Number of heads in the attention mechanism\n",
    "    #d_model    :Default is equal to gap\n",
    "    #num_classes:Number of cell types\n",
    "    #dropout    :dropout rate which is used to prevent model overfitting\n",
    "class CIForm(nn.Module):\n",
    "    def __init__(self, input_dim, nhead=2, d_model=80, num_classes=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, dim_feedforward=1024, nhead=nhead, dropout=dropout\n",
    "        )\n",
    "        self.positionalEncoding = PositionalEncoding(d_model=d_model, dropout=dropout)\n",
    "        self.pred_layer = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, mels):\n",
    "        out = mels.permute(1, 0, 2)\n",
    "        #positionalEncoding layer\n",
    "        out = self.positionalEncoding(out)\n",
    "        #Transformer Encoder layer layer\n",
    "        out = self.encoder_layer(out)\n",
    "        out = out.transpose(0, 1)\n",
    "        #pooling layer\n",
    "        out = out.mean(dim=1)\n",
    "        #classification layer\n",
    "        out = self.pred_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2fe108",
   "metadata": {},
   "outputs": [],
   "source": [
    "##main\n",
    "##parameters\n",
    "    #s                  :the length of a sub-vector\n",
    "    #referece_datapath  :the paths of referece datasets\n",
    "    #Train_names        :the names of referece datasets  \n",
    "    #Testdata_path      :the path pf test dataset\n",
    "    #Testdata_name      :the name of test dataset\n",
    "def main(s,referece_datapaths,Train_names,Testdata_path,Testdata_name):\n",
    "    gap = s           #the length of a sub-vector\n",
    "    topgenes = 2000   #the number of HVGs\n",
    "    d_models = s\n",
    "    heads = 64       #the number of heads in self-attention mechanism\n",
    "\n",
    "    lr = 0.0001      #learning rate\n",
    "    dp = 0.1         #dropout rate\n",
    "    batch_sizes = 256 #the size of batch\n",
    "    n_epochs = 20     #the number of epoch\n",
    "    \n",
    "    #Getting the data which input into the CIForm\n",
    "    train_data, labels, query_data, cell_types = getData(gap, referece_datapaths,Train_names,Testdata_path,\n",
    "                                             Testdata_name,topgenes)\n",
    "\n",
    "    #Number of cell types plus unassigned cell type\n",
    "    num_classes = np.unique(cell_types) + 1\n",
    " \n",
    "    #Constructing the CIForm model\n",
    "    model = CIForm(input_dim=d_models, nhead=heads, d_model=d_models,\n",
    "                       num_classes=num_classes,dropout=dp)\n",
    "\n",
    "    #Setting loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    #Setting optimization function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "\n",
    "    #Setting the training dataset\n",
    "    train_dataset = TrainDataSet(data=train_data, label=labels)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_sizes, shuffle=True,\n",
    "                              pin_memory=True)\n",
    "    #Setting the test dataset\n",
    "    test_dataset = TestDataSet(data=query_data)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_sizes, shuffle=False,\n",
    "                             pin_memory=True)\n",
    "    #Known Cell Types\n",
    "    new_cellTypes = []\n",
    "    new_cellTypes.append(\"unassigned\")\n",
    "    new_cellTypes.extend(cell_types)\n",
    "\n",
    "    \n",
    "    #startting training CIForm.Using training data to train CIForm\n",
    "    #n_epochs: the times of Training \n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        # These are used to record information in training.\n",
    "        train_loss = []\n",
    "        train_accs = []\n",
    "        train_f1s = []\n",
    "        for batch in tqdm(train_loader):\n",
    "            # A batch consists of scRNA-seq data and corresponding cell type annotation.\n",
    "            data, labels = batch\n",
    "            logits = model(data)\n",
    "            labels = torch.tensor(labels, dtype=torch.long)\n",
    "            loss = criterion(logits, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Getting the predicted cell type\n",
    "            preds = logits.argmax(1)\n",
    "            preds = preds.cpu().numpy()\n",
    "            labels = labels.cpu().numpy()\n",
    "            #Metrics\n",
    "            acc = accuracy_score(labels, preds)\n",
    "            f1 = f1_score(labels,preds,average='macro')\n",
    "            train_loss.append(loss.item())\n",
    "            train_accs.append(acc)\n",
    "            train_f1s.append(f1)\n",
    "        train_loss = sum(train_loss) / len(train_loss)\n",
    "        train_acc = sum(train_accs) / len(train_accs)\n",
    "        train_f1 = sum(train_f1s) / len(train_f1s)\n",
    "\n",
    "        print(f\"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}, f1 = {train_f1:.5f}\")\n",
    "        \n",
    "        ##Start the validation model, which predicts the cell types in the test dataset\n",
    "        model.eval()\n",
    "        test_accs = []\n",
    "        test_f1s = []\n",
    "        y_predict = []\n",
    "        labelss = []\n",
    "        for batch in tqdm(test_loader):\n",
    "            # A batch consists of scRNA-seq data and corresponding cell type annotations.\n",
    "            data, labels = batch\n",
    "            with torch.no_grad():\n",
    "                logits = model(data)\n",
    "            # Getting the predicted cell type\n",
    "            preds = logits.argmax(1)\n",
    "            preds = preds.cpu().numpy().tolist()\n",
    "            labels = labels.cpu().numpy().tolist()\n",
    "            \n",
    "            #Metrics\n",
    "            acc = accuracy_score(labels, preds)\n",
    "            f1 = f1_score(labels, preds, average='macro')\n",
    "            test_f1s.append(f1)\n",
    "            test_accs.append(acc)\n",
    "\n",
    "            y_predict.extend(preds)\n",
    "            labelss.extend(labels)\n",
    "        test_acc = sum(test_accs) / len(test_accs)\n",
    "        test_f1 = sum(test_f1s) / len(test_f1s)\n",
    "        print(\"---------------------------------------------end test---------------------------------------------\")\n",
    "        print(\"len(y_predict)\",len(y_predict))\n",
    "        all_acc = accuracy_score(labelss, y_predict)\n",
    "        all_f1 = f1_score(labelss, y_predict, average='macro')\n",
    "        print(\"all_acc:\", all_acc,\"all_f1:\", all_f1)\n",
    "\n",
    "        labelsss = []\n",
    "        y_predicts = []\n",
    "        for i in labelss:\n",
    "            labelsss.append(cell_types[i])\n",
    "        for i in y_predict:\n",
    "            y_predicts.append(cell_types[i])\n",
    "        \n",
    "        \n",
    "        #Storing predicted cell types and the CIForm\n",
    "        log_dir = \"log/\"\n",
    "        if (not os.path.isdir(log_dir)):\n",
    "            os.makedirs(log_dir)\n",
    "\n",
    "        np.save(log_dir  + 'y_tests.npy', labelsss)\n",
    "        np.save(log_dir  + 'y_predicts.npy', y_predicts)\n",
    "\n",
    "        torch.save(model.state_dict(), log_dir + 'CIForm.tar')\n",
    "        with open(log_dir + \"resilt.txt\", \"a\") as f:\n",
    "            f.writelines(\"acc:\" + str(all_acc) + \"\\n\")\n",
    "            f.writelines('f1:' + str(all_f1) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700553df",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 1024   # the length of a sub-vector\n",
    "\n",
    "x_Traindata_path1 = 'Dataset/pan/Baron Human/'\n",
    "Train_name1 = 'Baron Human'\n",
    "\n",
    "x_Traindata_path2 = 'Dataset/pan/Baron Mouse/'\n",
    "Train_name2 = 'Baron Mouse'\n",
    "\n",
    "x_Traindata_path3 = 'Dataset/pan/Muraro/'\n",
    "Train_name3 = 'Muraro'\n",
    "\n",
    "x_Traindata_path4 = 'Dataset/pan/Segerstolpe/'\n",
    "Train_name4 = 'Segerstolpe'\n",
    "\n",
    "referece_datapaths = [x_Traindata_path1,x_Traindata_path2,x_Traindata_path3,x_Traindata_path4]\n",
    "Train_names = [Train_name1,Train_name2,Train_name3,Train_name4]\n",
    "\n",
    "Testdata_path = 'Dataset/pan/Xin/'\n",
    "Testdata_name = 'Xin'\n",
    "##parameters\n",
    "    #s  : the length of a sub-vector\n",
    "    #referece_datapaths: the paths of referece datasets\n",
    "    #Train_names: the names of referece datasets  \n",
    "    #Testdata_path: the path pf test dataset\n",
    "    #Testdata_name: the name of test dataset\n",
    "main(s,referece_datapaths,Train_names,Testdata_path,Testdata_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
